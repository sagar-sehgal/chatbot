{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['']\n",
      "They do not!\n",
      "[['L194', 'L195', 'L196', 'L197'], ['L198', 'L199'], ['L200', 'L201', 'L202', 'L203'], ['L204', 'L205', 'L206'], ['L207', 'L208'], ['L271', 'L272', 'L273', 'L274', 'L275'], ['L276', 'L277'], ['L280', 'L281'], ['L363', 'L364'], ['L365', 'L366']]\n",
      "run\n",
      "cours \n",
      "run\n",
      "courez \n",
      "fire\n",
      "au feu \n",
      "help\n",
      "À l'aide \n",
      "jump\n",
      "saute\n",
      "stop\n",
      "Ça suffit \n",
      "stop\n",
      "stop \n",
      "stop\n",
      "arrêtetoi \n",
      "wait\n",
      "attends \n",
      "length of input_x_clean is\n",
      "154883\n",
      "length of output_x_clean is\n",
      "154883\n",
      "length of final input is\n",
      "154239\n",
      "length of final output is\n",
      "154239\n",
      "length of vovabulary is\n",
      "14235\n",
      "31348\n",
      "length of vocab to int is\n",
      "2460\n",
      "1938\n",
      "length of int to vocab is\n",
      "1938\n",
      "2460\n",
      "<GO> je comprends <EOS>\n",
      "length of output in and input int\n",
      "154239\n",
      "154239\n",
      "checking output int and input int\n",
      "[924, 809]\n",
      "[3, 1164, 1596, 2, 1]\n",
      "maximum length of a line is for input\n",
      "20\n",
      "[924, 332]\n",
      "min length of a line is for input\n",
      "2\n",
      "length of sorted input is\n",
      "154239\n",
      "154239\n",
      "length of sorted output is\n",
      "[3, 1027, 2129, 1]\n",
      "[3, 1164, 1596, 2, 1]\n",
      "[3, 1027, 1835, 2, 2, 1]\n",
      "[3, 2, 221, 2, 1]\n",
      "shape of encoder and decoder input is\n",
      "(154239, 20)\n",
      "(154239, 22)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, GRU, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import LSTM, Input\n",
    "tf.__version__\n",
    "\n",
    "lines = open('cornell/movie_lines.txt').read().split('\\n')\n",
    "len(lines)\n",
    "\n",
    "\n",
    "lines[:20]\n",
    "#lineid, #characterID , #movieID , #chracter name , #text of the utterance\n",
    "\n",
    "conv_lines = open('cornell/movie_conversations.txt').read().split('\\n')\n",
    "conv_lines[:10]\n",
    "#characterid 1, characterid 2 ,movie id, lineids in chronological order\n",
    "\n",
    "id2line = {}\n",
    "for line in lines:\n",
    "    line_list = line.split(' +++$+++ ')\n",
    "    if len(line_list) == 5:\n",
    "        id2line[line_list[0]] = line_list[4]\n",
    "    else :\n",
    "        print line_list\n",
    "\n",
    "print id2line['L1045']\n",
    "\n",
    "convs = []\n",
    "for line in conv_lines[:-1]:\n",
    "    _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \"\").replace(\" \",\"\")\n",
    "    convs.append(_line.split(','))\n",
    "\n",
    "print convs[:10]\n",
    "\n",
    "input_x = []\n",
    "output_y = []\n",
    "for conv in convs:\n",
    "    for i in range(len(conv) - 1):\n",
    "        input_x.append(id2line[conv[i]])\n",
    "        output_y.append(id2line[conv[i+1]])\n",
    "\n",
    "#for i in range(1,10):\n",
    "#    print(input_x[i])\n",
    "#    print(output_y[i])\n",
    "##change from here\n",
    "inputen_x = []\n",
    "outputfr_y = []\n",
    "with open(\"fra.txt\") as f:\n",
    "    for line in f:\n",
    "        values = line.split('\\t')\n",
    "        inputen_x.append(values[0].rstrip())\n",
    "        outputfr_y.append(values[1].rstrip())\n",
    "\n",
    "\n",
    "#for i in range(1,10):\n",
    "#    print inputen_x[i]\n",
    "#    print outputfr_y[i]\n",
    "\n",
    "##to here\n",
    "\n",
    "    \n",
    "def clean_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "input_x_clean = []\n",
    "for i in range(len(inputen_x)):\n",
    "    input_x_clean.append(clean_text(inputen_x[i]))\n",
    "\n",
    "output_y_clean = []\n",
    "for j in range(len(outputfr_y)):\n",
    "    output_y_clean.append(clean_text(outputfr_y[j]))\n",
    "    \n",
    "for i in range(1,10):\n",
    "    print(input_x_clean[i])\n",
    "    print(output_y_clean[i])\n",
    "print (\"length of input_x_clean is\")\n",
    "print(len(input_x_clean))\n",
    "print (\"length of output_x_clean is\")\n",
    "print(len(output_y_clean))\n",
    "#input_x_clean = [:10000]\n",
    "#output_y_clean = [:10000]\n",
    "min_length = 2\n",
    "max_length = 20\n",
    "final_input = []\n",
    "final_output = []\n",
    "for i in range(0,len(input_x_clean)):\n",
    "    temp_input_list = input_x_clean[i].split(' ')\n",
    "    temp_output_list = output_y_clean[i].split(' ')\n",
    "    if len(temp_input_list) > max_length or len(temp_input_list) < min_length  or len(temp_output_list) > max_length or len(temp_output_list) < min_length:\n",
    "        d  = 1 + 1\n",
    "    else:\n",
    "        final_input.append(input_x_clean[i])\n",
    "        final_output.append(output_y_clean[i])\n",
    "    \n",
    "print (\"length of final input is\")\n",
    "print (len(final_input))\n",
    "print (\"length of final output is\")\n",
    "print (len(final_output))\n",
    "\n",
    "\n",
    "\n",
    "vocab_en = {}\n",
    "for sent in final_input:\n",
    "    for word in sent.split():\n",
    "        if word not in vocab_en:\n",
    "            vocab_en[word] = 1\n",
    "        else:\n",
    "            vocab_en[word] += 1\n",
    "\n",
    "vocab_fr = {}\n",
    "for sent in final_output:\n",
    "    for word in sent.split():\n",
    "        if word not in vocab_fr:\n",
    "            vocab_fr[word] = 1\n",
    "        else:\n",
    "            vocab_fr[word] += 1\n",
    "\n",
    "print(\"length of vovabulary is\")\n",
    "print len(vocab_en)\n",
    "print len(vocab_fr)\n",
    "\n",
    "word_num = 0\n",
    "threshold = 30\n",
    "codes  = ['<EOS>', '<UNK>', '<GO>']\n",
    "vocab_to_int_output = {}\n",
    "vocab_to_int_output['<PAD>'] = 0\n",
    "for code in codes:\n",
    "    vocab_to_int_output[code] = len(vocab_to_int_output)\n",
    "for word, count in vocab_fr.items():\n",
    "    if count >= threshold:\n",
    "        vocab_to_int_output[word] = word_num+4\n",
    "        word_num += 1\n",
    "\n",
    "vocab_to_int_input = {}\n",
    "for word, count in vocab_en.items():\n",
    "    if count >= threshold:\n",
    "        vocab_to_int_input[word] = len(vocab_to_int_input)\n",
    "vocab_to_int_input['<UNK>'] = len(vocab_to_int_input)    \n",
    "print (\"length of vocab to int is\")\n",
    "print(len(vocab_to_int_output))\n",
    "print (len(vocab_to_int_input))\n",
    "\n",
    "int_to_vocab_input = {v_i : v for v, v_i in vocab_to_int_input.items()}\n",
    "int_to_vocab_output = {v_i : v for v, v_i in vocab_to_int_output.items()}\n",
    "print (\"length of int to vocab is\")\n",
    "print(len(int_to_vocab_input))\n",
    "print(len(int_to_vocab_output))\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(final_output)):\n",
    "    final_output[i] += ' <EOS>'\n",
    "for i in range(len(final_output)):\n",
    "    final_output[i] = '<GO> ' + final_output[i]\n",
    "print (final_output[0])\n",
    "\n",
    "input_int = []\n",
    "for sent in final_input:\n",
    "    temp_int = []\n",
    "    for word in sent.split( ' ' ):\n",
    "        if word in vocab_to_int_input:\n",
    "            temp_int.append(vocab_to_int_input[word])\n",
    "        else:\n",
    "            temp_int.append(vocab_to_int_input['<UNK>'])\n",
    "    input_int.append(temp_int)\n",
    "output_int = []\n",
    "for sent in final_output:\n",
    "    temp_int = []\n",
    "    for word in sent.split( ' ' ):\n",
    "        if word in vocab_to_int_output:\n",
    "            temp_int.append(vocab_to_int_output[word])\n",
    "        else:\n",
    "            temp_int.append(vocab_to_int_output['<UNK>'])\n",
    "    output_int.append(temp_int)\n",
    "    \n",
    "print (\"length of output in and input int\")\n",
    "print (len(input_int))\n",
    "print (len(output_int))\n",
    "print (\"checking output int and input int\")\n",
    "print (input_int[1])\n",
    "print (output_int[1])\n",
    "\n",
    "max_length = 0\n",
    "for i in range (len(input_int)):\n",
    "    if len(input_int[i]) > max_length:\n",
    "        max_length = len(input_int[i])\n",
    "\n",
    "print (\"maximum length of a line is for input\")\n",
    "print (max_length)\n",
    "\n",
    "import math\n",
    "min_length = float('inf')\n",
    "for i in range (len(input_int)):\n",
    "    if len(input_int[i]) < min_length:\n",
    "        min_length = len(input_int[i])\n",
    "        print (input_int[i])\n",
    "\n",
    "print (\"min length of a line is for input\")\n",
    "print (min_length)\n",
    "\n",
    "sorted_input = []\n",
    "sorted_output = []\n",
    "for length in range(min_length, max_length+1):\n",
    "    for i in enumerate(input_int):\n",
    "        if len(i[1]) == length:\n",
    "            sorted_input.append(input_int[i[0]])\n",
    "            sorted_output.append(output_int[i[0]])\n",
    "            \n",
    "print (\"length of sorted input is\")\n",
    "print (len(sorted_input))\n",
    "print (len(sorted_output))\n",
    "print (\"length of sorted output is\")\n",
    "\n",
    "for i in range(4):\n",
    "    print sorted_output[i]\n",
    "    \n",
    "## PAD is 0 , EOS is 1 , UNK is 2 and GO is 3\n",
    "\n",
    "np_encoder_input = np.zeros([len(sorted_input), len(max(sorted_input, key = lambda x : len(x)))], dtype = np.int32)\n",
    "for i,j in enumerate(sorted_input):\n",
    "    np_encoder_input[i][0:len(j)] = j\n",
    "np_decoder_input = np.zeros([len(sorted_output), len(max(sorted_output, key = lambda x : len(x)))], dtype = np.int32)\n",
    "for i,j in enumerate(sorted_output):\n",
    "    np_decoder_input[i][0:len(j)] = j\n",
    "    \n",
    "print (\"shape of encoder and decoder input is\")\n",
    "print (np_encoder_input.shape)\n",
    "print (np_decoder_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder_input  = [num_of_sentences, length]  , decoder_input = [number_of_sentences, length] \n",
    "#the vocablury size is 7878 the sequence length for encoder is 20 while for decoder its 22.\n",
    "#Now we will make embedding layer for incoder and decoder \n",
    "#Input shape\n",
    "\n",
    "#2D tensor with shape: (batch_size, sequence_length).\n",
    "\n",
    "#Output shape\n",
    "\n",
    "\n",
    "#3D tensor with shape: (batch_size, sequence_length, output_dim).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.7.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, GRU, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import LSTM, Input\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 7)\n",
      "(100000, 5)\n",
      "[16  4 11 20 25]\n",
      "[ 0 16  4 11 20 25 30]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "n_data = 100000\n",
    "input_len = 5\n",
    "output_len = 7\n",
    "output_data = np.zeros((n_data,output_len),dtype=np.int32)\n",
    "input_data = np.zeros((n_data, input_len), dtype = np.int32)\n",
    "print output_data.shape\n",
    "print input_data.shape\n",
    "for i in range(n_data):\n",
    "    for j in range(input_len):\n",
    "        input_data[i][j] = random.randint(3,30)\n",
    "        \n",
    "for i in range(n_data):\n",
    "    output_data[i][0] = 0\n",
    "    output_data[i][6] = 30\n",
    "    for j in range(1,6):\n",
    "        output_data[i][j] = input_data[i][j-1]\n",
    "\n",
    "        \n",
    "print input_data[0]\n",
    "print output_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size_input = 31\n",
    "vocab_size_output = 31\n",
    "encoder_seq_length = 5\n",
    "decoder_seq_length = 7\n",
    "latent_dim = 32\n",
    "num_encoder_tokens = vocab_size_input\n",
    "num_decoder_tokens = vocab_size_output\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "total_data = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np_encoder_input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3d8e1f8dee9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp_encoder_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp_decoder_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mconv_lines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np_encoder_input' is not defined"
     ]
    }
   ],
   "source": [
    "print (np_encoder_input[1])\n",
    "print (np_decoder_input[1])\n",
    "import gc\n",
    "del conv_lines\n",
    "del lines\n",
    "del convs\n",
    "del input_x, output_y\n",
    "del input_x_clean, output_y_clean\n",
    "del final_input,final_output\n",
    "del input_int, output_int\n",
    "del sorted_input,sorted_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 6)\n",
      "(100000, 5)\n"
     ]
    }
   ],
   "source": [
    "np_encoder_input = input_data\n",
    "np_decoder_input = output_data\n",
    "decoder_input_data = np_decoder_input[:, :-1]\n",
    "decoder_target_data = np_decoder_input[:, 1:]\n",
    "encoder_input_data = np.copy(np_encoder_input)\n",
    "print (decoder_input_data.shape)\n",
    "print (encoder_input_data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 5)\n",
      "(100000, 6)\n",
      "(100000, 6)\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(encoder_input_data.shape)\n",
    "print(decoder_input_data.shape)\n",
    "print(decoder_target_data.shape)\n",
    "max= 0 \n",
    "for dataarray in decoder_target_data:\n",
    "    for data in dataarray:\n",
    "        if max < data:\n",
    "            max=data\n",
    "\n",
    "print max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def generate_data(encoder_input_data,decoder_input_data,decoder_target_data):\n",
    "    batch_size = 512\n",
    "    re_encoder_input = np.zeros((512,5), dtype = np.int32)\n",
    "    re_decoder_input = np.zeros((512,6), dtype= np.int32)\n",
    "    re_decoder_target = np.zeros((512,6,31), dtype= np.int32)\n",
    "    counter = 0\n",
    "    max_i = len(encoder_input_data)\n",
    "    while True:\n",
    "        while counter < batch_size :\n",
    "            i = random.randint(0,max_i)\n",
    "            re_encoder_input[counter] = encoder_input_data[i][:]\n",
    "            re_decoder_input[counter] = decoder_input_data[i][:]\n",
    "            k = 0\n",
    "            while k < len(decoder_target_data[i]):\n",
    "                re_decoder_target[counter][k][decoder_target_data[i][k]] = 1\n",
    "                k += 1\n",
    "            counter += 1\n",
    "        \n",
    "        yield [re_encoder_input, re_decoder_input] , re_decoder_target\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "194/194 [==============================] - 143s 739ms/step - loss: 2.5211\n",
      "Epoch 2/100\n",
      "194/194 [==============================] - 144s 742ms/step - loss: 1.2695\n",
      "Epoch 3/100\n",
      "194/194 [==============================] - 145s 747ms/step - loss: 0.3531\n",
      "Epoch 4/100\n",
      "194/194 [==============================] - 145s 746ms/step - loss: 0.0664\n",
      "Epoch 5/100\n",
      "194/194 [==============================] - 158s 813ms/step - loss: 0.0430\n",
      "Epoch 6/100\n",
      "194/194 [==============================] - 162s 837ms/step - loss: 0.0099\n",
      "Epoch 7/100\n",
      "194/194 [==============================] - 147s 756ms/step - loss: 0.0117\n",
      "Epoch 8/100\n",
      "194/194 [==============================] - 143s 738ms/step - loss: 0.0105\n",
      "Epoch 9/100\n",
      "194/194 [==============================] - 143s 739ms/step - loss: 0.0062\n",
      "Epoch 10/100\n",
      "194/194 [==============================] - 145s 746ms/step - loss: 1.9135e-06\n",
      "Epoch 11/100\n",
      "194/194 [==============================] - 166s 854ms/step - loss: 5.4843e-07\n",
      "Epoch 12/100\n",
      "194/194 [==============================] - 158s 815ms/step - loss: 3.2850e-07\n",
      "Epoch 13/100\n",
      "194/194 [==============================] - 172s 885ms/step - loss: 2.3936e-07\n",
      "Epoch 14/100\n",
      "194/194 [==============================] - 156s 802ms/step - loss: 1.9236e-07\n",
      "Epoch 15/100\n",
      "194/194 [==============================] - 161s 829ms/step - loss: 1.6527e-07\n",
      "Epoch 16/100\n",
      "194/194 [==============================] - 160s 824ms/step - loss: 1.4832e-07\n",
      "Epoch 17/100\n",
      "194/194 [==============================] - 174s 899ms/step - loss: 1.3808e-07\n",
      "Epoch 18/100\n",
      "194/194 [==============================] - 176s 909ms/step - loss: 1.3083e-07\n",
      "Epoch 19/100\n",
      "194/194 [==============================] - 171s 883ms/step - loss: 1.2656e-07\n",
      "Epoch 20/100\n",
      "194/194 [==============================] - 181s 934ms/step - loss: 1.2381e-07\n",
      "Epoch 21/100\n",
      "194/194 [==============================] - 176s 909ms/step - loss: 1.2222e-07\n",
      "Epoch 22/100\n",
      "194/194 [==============================] - 188s 969ms/step - loss: 1.2121e-07\n",
      "Epoch 23/100\n",
      "194/194 [==============================] - 229s 1s/step - loss: 1.2052e-07\n",
      "Epoch 24/100\n",
      "194/194 [==============================] - 247s 1s/step - loss: 1.2006e-07\n",
      "Epoch 25/100\n",
      "194/194 [==============================] - 289s 1s/step - loss: 1.1983e-07\n",
      "Epoch 26/100\n",
      "194/194 [==============================] - 351s 2s/step - loss: 1.1958e-07\n",
      "Epoch 27/100\n",
      "194/194 [==============================] - 475s 2s/step - loss: 1.1950e-07\n",
      "Epoch 28/100\n",
      "194/194 [==============================] - 169s 870ms/step - loss: 1.1940e-07\n",
      "Epoch 29/100\n",
      "194/194 [==============================] - 144s 743ms/step - loss: 1.1934e-07\n",
      "Epoch 30/100\n",
      "194/194 [==============================] - 144s 741ms/step - loss: 1.1932e-07\n",
      "Epoch 31/100\n",
      "194/194 [==============================] - 144s 742ms/step - loss: 1.1929e-07\n",
      "Epoch 32/100\n",
      "194/194 [==============================] - 144s 741ms/step - loss: 1.1926e-07\n",
      "Epoch 33/100\n",
      "194/194 [==============================] - 144s 742ms/step - loss: 1.1926e-07\n",
      "Epoch 34/100\n",
      "194/194 [==============================] - 144s 741ms/step - loss: 1.1926e-07\n",
      "Epoch 35/100\n",
      "194/194 [==============================] - 144s 740ms/step - loss: 1.1922e-07\n",
      "Epoch 36/100\n",
      "194/194 [==============================] - 143s 739ms/step - loss: 1.1922e-07\n",
      "Epoch 37/100\n",
      "194/194 [==============================] - 143s 740ms/step - loss: 1.1922e-07\n",
      "Epoch 38/100\n",
      "194/194 [==============================] - 144s 741ms/step - loss: 1.1921e-07\n",
      "Epoch 39/100\n",
      "194/194 [==============================] - 144s 740ms/step - loss: 1.1921e-07\n",
      "Epoch 40/100\n",
      "194/194 [==============================] - 144s 740ms/step - loss: 1.1921e-07\n",
      "Epoch 41/100\n",
      "194/194 [==============================] - 143s 739ms/step - loss: 1.1921e-07\n",
      "Epoch 42/100\n",
      "194/194 [==============================] - 143s 739ms/step - loss: 1.1921e-07\n",
      "Epoch 43/100\n",
      "194/194 [==============================] - 144s 740ms/step - loss: 1.1921e-07\n",
      "Epoch 44/100\n",
      "194/194 [==============================] - 144s 740ms/step - loss: 1.1921e-07\n",
      "Epoch 45/100\n",
      "194/194 [==============================] - 144s 743ms/step - loss: 1.1921e-07\n",
      "Epoch 46/100\n",
      "194/194 [==============================] - 143s 740ms/step - loss: 1.1921e-07\n",
      "Epoch 47/100\n",
      "194/194 [==============================] - 143s 737ms/step - loss: 1.1921e-07\n",
      "Epoch 48/100\n",
      "194/194 [==============================] - 143s 738ms/step - loss: 1.1921e-07\n",
      "Epoch 49/100\n",
      "194/194 [==============================] - 143s 737ms/step - loss: 1.1921e-07\n",
      "Epoch 50/100\n",
      "194/194 [==============================] - 145s 747ms/step - loss: 1.1921e-07\n",
      "Epoch 51/100\n",
      "194/194 [==============================] - 143s 737ms/step - loss: 1.1921e-07\n",
      "Epoch 52/100\n",
      "194/194 [==============================] - 143s 737ms/step - loss: 1.1921e-07\n",
      "Epoch 53/100\n",
      "194/194 [==============================] - 143s 735ms/step - loss: 1.1921e-07\n",
      "Epoch 54/100\n",
      "194/194 [==============================] - 143s 736ms/step - loss: 1.1921e-07\n",
      "Epoch 55/100\n",
      "194/194 [==============================] - 143s 736ms/step - loss: 1.1921e-07\n",
      "Epoch 56/100\n",
      "194/194 [==============================] - 143s 736ms/step - loss: 1.1921e-07\n",
      "Epoch 57/100\n",
      "194/194 [==============================] - 143s 735ms/step - loss: 1.1921e-07\n",
      "Epoch 58/100\n",
      "194/194 [==============================] - 143s 736ms/step - loss: 1.1921e-07\n",
      "Epoch 59/100\n",
      "194/194 [==============================] - 143s 735ms/step - loss: 1.1921e-07\n",
      "Epoch 60/100\n",
      "194/194 [==============================] - 143s 735ms/step - loss: 1.1921e-07\n",
      "Epoch 61/100\n",
      "194/194 [==============================] - 143s 735ms/step - loss: 1.1921e-07\n",
      "Epoch 62/100\n",
      "194/194 [==============================] - 143s 735ms/step - loss: 1.1921e-07\n",
      "Epoch 63/100\n",
      "194/194 [==============================] - 143s 736ms/step - loss: 1.1921e-07\n",
      "Epoch 64/100\n",
      "194/194 [==============================] - 143s 736ms/step - loss: 1.1921e-07\n",
      "Epoch 65/100\n",
      "194/194 [==============================] - 143s 736ms/step - loss: 1.1921e-07\n",
      "Epoch 66/100\n",
      "194/194 [==============================] - 143s 738ms/step - loss: 1.1921e-07\n",
      "Epoch 67/100\n",
      "194/194 [==============================] - 143s 735ms/step - loss: 1.1921e-07\n",
      "Epoch 68/100\n",
      "194/194 [==============================] - 147s 758ms/step - loss: 1.1921e-07\n",
      "Epoch 69/100\n",
      "194/194 [==============================] - 158s 815ms/step - loss: 1.1921e-07\n",
      "Epoch 70/100\n",
      "194/194 [==============================] - 159s 818ms/step - loss: 1.1921e-07\n",
      "Epoch 71/100\n",
      "194/194 [==============================] - 168s 865ms/step - loss: 1.1921e-07\n",
      "Epoch 72/100\n",
      "194/194 [==============================] - 163s 839ms/step - loss: 1.1921e-07\n",
      "Epoch 73/100\n",
      "194/194 [==============================] - 158s 815ms/step - loss: 1.1921e-07\n",
      "Epoch 74/100\n",
      "194/194 [==============================] - 158s 815ms/step - loss: 1.1921e-07\n",
      "Epoch 75/100\n",
      "194/194 [==============================] - 158s 813ms/step - loss: 1.1921e-07\n",
      "Epoch 76/100\n",
      "194/194 [==============================] - 160s 823ms/step - loss: 1.1921e-07\n",
      "Epoch 77/100\n",
      "194/194 [==============================] - 164s 846ms/step - loss: 1.1921e-07\n",
      "Epoch 78/100\n",
      "194/194 [==============================] - 161s 828ms/step - loss: 1.1921e-07\n",
      "Epoch 79/100\n",
      "194/194 [==============================] - 163s 840ms/step - loss: 1.1921e-07\n",
      "Epoch 80/100\n",
      "194/194 [==============================] - 163s 842ms/step - loss: 1.1921e-07\n",
      "Epoch 81/100\n",
      "194/194 [==============================] - 161s 830ms/step - loss: 1.1921e-07\n",
      "Epoch 82/100\n",
      "194/194 [==============================] - 163s 839ms/step - loss: 1.1921e-07\n",
      "Epoch 83/100\n",
      "194/194 [==============================] - 157s 810ms/step - loss: 1.1921e-07\n",
      "Epoch 84/100\n",
      "194/194 [==============================] - 149s 768ms/step - loss: 1.1921e-07\n",
      "Epoch 85/100\n",
      "194/194 [==============================] - 165s 848ms/step - loss: 1.1921e-07\n",
      "Epoch 86/100\n",
      "194/194 [==============================] - 168s 866ms/step - loss: 1.1921e-07\n",
      "Epoch 87/100\n",
      "194/194 [==============================] - 165s 852ms/step - loss: 1.1921e-07\n",
      "Epoch 88/100\n",
      "194/194 [==============================] - 164s 848ms/step - loss: 1.1921e-07\n",
      "Epoch 89/100\n",
      "194/194 [==============================] - 165s 852ms/step - loss: 1.1921e-07\n",
      "Epoch 90/100\n",
      "194/194 [==============================] - 167s 863ms/step - loss: 1.1921e-07\n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194/194 [==============================] - 162s 833ms/step - loss: 1.1921e-07\n",
      "Epoch 92/100\n",
      "194/194 [==============================] - 163s 840ms/step - loss: 1.1921e-07\n",
      "Epoch 93/100\n",
      "194/194 [==============================] - 161s 832ms/step - loss: 1.1921e-07\n",
      "Epoch 94/100\n",
      "194/194 [==============================] - 163s 841ms/step - loss: 1.1921e-07\n",
      "Epoch 95/100\n",
      "194/194 [==============================] - 165s 848ms/step - loss: 1.1921e-07\n",
      "Epoch 96/100\n",
      "194/194 [==============================] - 163s 840ms/step - loss: 1.1921e-07\n",
      "Epoch 97/100\n",
      "194/194 [==============================] - 167s 860ms/step - loss: 1.1921e-07\n",
      "Epoch 98/100\n",
      "194/194 [==============================] - 164s 846ms/step - loss: 1.1921e-07\n",
      "Epoch 99/100\n",
      "194/194 [==============================] - 162s 835ms/step - loss: 1.1921e-07\n",
      "Epoch 100/100\n",
      "194/194 [==============================] - 161s 832ms/step - loss: 1.1921e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f151d21c850>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#latent_dim = 64\n",
    "#num_encoder_tokens = 374\n",
    "#num_decoder_tokens = 374\n",
    "#epochs = 10\n",
    "#batch_size = 64\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "x = Embedding(num_encoder_tokens, latent_dim)(encoder_inputs)\n",
    "x, state_h, state_c = LSTM(512,\n",
    "                           return_state=True)(x)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embedding_x = Embedding(num_decoder_tokens, latent_dim)(decoder_inputs)\n",
    "decoder_lstm = LSTM(512, return_sequences=True, return_state=True)\n",
    "\n",
    "x,_,_ = decoder_lstm(decoder_embedding_x, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(x)\n",
    "\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "model.fit_generator(generate_data(encoder_input_data, decoder_input_data, decoder_target_data),\n",
    "          steps_per_epoch = 194,epochs = 100\n",
    "          \n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(512,))\n",
    "decoder_state_input_c = Input(shape=(512,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_embedding_x, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, None, 32)          992       \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                [(None, 512), (None, 512) 1116160   \n",
      "=================================================================\n",
      "Total params: 1,117,152\n",
      "Trainable params: 1,117,152\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, None, 32)     992         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   [(None, None, 512),  1116160     embedding_6[0][0]                \n",
      "                                                                 input_9[0][0]                    \n",
      "                                                                 input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, None, 31)     15903       lstm_6[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,133,055\n",
      "Trainable params: 1,133,055\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model.summary()\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_seq_to_num(sent):\n",
    "    output_sent = np.zeros((1,20),dtype=np.int32)\n",
    "    initial_list = []\n",
    "    sent = clean_text(sent)\n",
    "    for words in sent.split(' '):\n",
    "        if words in vocab_to_int_input:\n",
    "            initial_list.append(vocab_to_int_input[words])\n",
    "        else:\n",
    "            initial_list.append(vocab_to_int_input['<UNK>'])\n",
    "            \n",
    "    output_sent[0][0:len(initial_list)] = initial_list\n",
    "    output_sent[0] = output_sent[0][::-1]\n",
    "    return output_sent\n",
    "    \n",
    "#input_seq =  input_seq_to_num(\"hello how are you\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#states_value = encoder_model.predict(input_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (states_value[0].shape)\n",
    "#print (states_value[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speak(input_sent):\n",
    "    states_value = encoder_model.predict(input_sent)\n",
    "    encoder_input_seq =  np.zeros((1,1), dtype=np.int32)\n",
    "    encoder_input_seq[0][0]  = 0\n",
    "    stop_condition = False\n",
    "    decoded_sent = []\n",
    "    max_decoder_seq_length = 10\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([encoder_input_seq] + states_value)\n",
    "\n",
    "        \n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        print sampled_token_index\n",
    "        sampled_char = sampled_token_index\n",
    "        decoded_sent.append(sampled_char)\n",
    "        if (sampled_char == 30 or len(decoded_sent) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "        encoder_input_seq =  np.zeros((1,1), dtype=np.int32)\n",
    "        encoder_input_seq[0][0]  = sampled_token_index\n",
    "        states_value = [h, c]\n",
    "    return \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "21\n",
      "3\n",
      "21\n",
      "5\n",
      "30\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print (speak([21,14 ,18 ,11, 11,  5, 29,  19 , 18, 26,  15, 23, 17, 12, 18, 14,  16, 13,  0,  0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "20\n",
      "22\n",
      "20\n",
      "30\n",
      "None\n",
      "30\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print (speak([19 ,25  ,4  ,4  ,3 ,29 ,22  ,4 ,30  ,0  ,0  ,0  ,0  ,0  ,0  ,0  ,0  ,0  ,0  ,0]))\n",
    "print (speak([26 ,30 ,19 ,28 ,14 ,18 ,26  ,0  ,0  ,0  ,0  ,0  ,0  ,0  ,0  ,0  ,0  ,0  ,0  ,0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
