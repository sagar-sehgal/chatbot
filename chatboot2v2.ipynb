{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.7.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, GRU, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import LSTM, Input, Bidirectional\n",
    "from keras.layers import Concatenate\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 7)\n",
      "(1000, 5)\n",
      "[20 25 11  3 27]\n",
      "[ 0 20 25 11  3 27 30]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "n_data = 1000\n",
    "input_len = 5\n",
    "output_len = 7\n",
    "output_data = np.zeros((n_data,output_len),dtype=np.int32)\n",
    "input_data = np.zeros((n_data, input_len), dtype = np.int32)\n",
    "print output_data.shape\n",
    "print input_data.shape\n",
    "for i in range(n_data):\n",
    "    for j in range(input_len):\n",
    "        input_data[i][j] = random.randint(3,30)\n",
    "        \n",
    "for i in range(n_data):\n",
    "    output_data[i][0] = 0\n",
    "    output_data[i][6] = 30\n",
    "    for j in range(1,6):\n",
    "        output_data[i][j] = input_data[i][j-1]\n",
    "\n",
    "        \n",
    "print input_data[0]\n",
    "print output_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size_input = 31\n",
    "vocab_size_output = 31\n",
    "encoder_seq_length = 5\n",
    "decoder_seq_length = 7\n",
    "latent_dim = 8\n",
    "num_encoder_tokens = vocab_size_input\n",
    "num_decoder_tokens = vocab_size_output\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "total_data = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 6)\n",
      "(1000, 5)\n"
     ]
    }
   ],
   "source": [
    "np_encoder_input = input_data\n",
    "np_decoder_input = output_data\n",
    "decoder_input_data = np_decoder_input[:, :-1]\n",
    "decoder_target_data = np_decoder_input[:, 1:]\n",
    "encoder_input_data = np.copy(np_encoder_input)\n",
    "print (decoder_input_data.shape)\n",
    "print (encoder_input_data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 5)\n",
      "(1000, 6)\n",
      "(1000, 6)\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(encoder_input_data.shape)\n",
    "print(decoder_input_data.shape)\n",
    "print(decoder_target_data.shape)\n",
    "max= 0 \n",
    "for dataarray in decoder_target_data:\n",
    "    for data in dataarray:\n",
    "        if max < data:\n",
    "            max=data\n",
    "\n",
    "print max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def generate_data(encoder_input_data,decoder_input_data,decoder_target_data):\n",
    "    batch_size = 500\n",
    "    re_encoder_input = np.zeros((500,5), dtype = np.int32)\n",
    "    re_decoder_input = np.zeros((500,6), dtype= np.int32)\n",
    "    re_decoder_target = np.zeros((500,6,31), dtype= np.int32)\n",
    "    counter = 0\n",
    "    max_i = len(encoder_input_data)\n",
    "    while True:\n",
    "        while counter < batch_size :\n",
    "            i = random.randint(0,max_i)\n",
    "            re_encoder_input[counter] = encoder_input_data[i][:]\n",
    "            re_decoder_input[counter] = decoder_input_data[i][:]\n",
    "            k = 0\n",
    "            while k < len(decoder_target_data[i]):\n",
    "                re_decoder_target[counter][k][decoder_target_data[i][k]] = 1\n",
    "                k += 1\n",
    "            counter += 1\n",
    "        \n",
    "        yield [re_encoder_input, re_decoder_input] , re_decoder_target\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 4s 2s/step - loss: 3.4316\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 1s 325ms/step - loss: 3.4251\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 1s 349ms/step - loss: 3.4193\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 1s 317ms/step - loss: 3.4132\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 1s 321ms/step - loss: 3.4067\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 1s 322ms/step - loss: 3.3995\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 1s 309ms/step - loss: 3.3915\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 3.3824\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 3.3721\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 3.3603\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 3.3468\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 3.3313\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 3.3135\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 3.2933\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 3.2705\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 3.2452\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 3.2182\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 1s 261ms/step - loss: 3.1903\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 1s 327ms/step - loss: 3.1630\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 1s 324ms/step - loss: 3.1378\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 1s 331ms/step - loss: 3.1162\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 1s 385ms/step - loss: 3.0986\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 1s 405ms/step - loss: 3.0850\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 1s 332ms/step - loss: 3.0747\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 219ms/step - loss: 3.0666\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 3.0599\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 3.0539\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 3.0485\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 3.0433\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 3.0384\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 3.0335\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 3.0288\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 3.0242\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 3.0196\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 3.0150\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 3.0106\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 3.0061\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 3.0017\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 2.9973\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 2.9930\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 2.9887\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 2.9844\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 2.9801\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 2.9759\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 2.9717\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 2.9675\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 2.9633\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 2.9591\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 2.9550\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 2.9509\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 2.9484\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 2.9446\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 2.9403\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 2.9366\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 2.9331\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 2.9298\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 2.9269\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 2.9240\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 2.9204\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 2.9169\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 2.9137\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 2.9107\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 2.9080\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 2.9052\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 2.9020\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 2.8989\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 2.8960\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 2.8932\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 2.8906\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 2.8879\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 2.8850\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 2.8822\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 2.8795\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 2.8769\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 2.8744\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 2.8718\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 2.8692\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 2.8666\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 2.8641\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 2.8616\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 2.8592\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 2.8568\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 2.8543\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 2.8518\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 2.8494\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 2.8471\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 2.8448\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 2.8425\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 2.8401\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 2.8378\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 2.8355\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 2.8332\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 2.8310\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 2.8288\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 2.8265\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 2.8243\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 2.8220\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 2.8199\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 2.8177\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 2.8155\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd027788c90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#latent_dim = 64\n",
    "#num_encoder_tokens = 374\n",
    "#num_decoder_tokens = 374\n",
    "#epochs = 10\n",
    "#batch_size = 64\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "x = Embedding(num_encoder_tokens, latent_dim)(encoder_inputs)\n",
    "x, forward_h, forward_c, backward_h, backward_c = Bidirectional(LSTM(8, return_state=True))(x)\n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embedding_x = Embedding(num_decoder_tokens, latent_dim)(decoder_inputs)\n",
    "decoder_lstm = LSTM(16, return_sequences=True, return_state=True)\n",
    "\n",
    "x,_,_ = decoder_lstm(decoder_embedding_x, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(x)\n",
    "\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "model.fit_generator(generate_data(encoder_input_data, decoder_input_data, decoder_target_data),\n",
    "          steps_per_epoch = 2,epochs = 100\n",
    "          \n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Graph disconnected: cannot obtain value for tensor Tensor(\"input_5:0\", shape=(?, ?), dtype=float32) at layer \"input_5\". The following previous layers were accessed without issue: []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-34507f5097c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m decoder_model = Model(\n\u001b[1;32m     11\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mdecoder_inputs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdecoder_states_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     [decoder_outputs] + decoder_states)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, name)\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0;34m'The following previous layers '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1810\u001b[0m                                 \u001b[0;34m'were accessed without issue: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1811\u001b[0;31m                                 str(layers_with_complete_input))\n\u001b[0m\u001b[1;32m   1812\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1813\u001b[0m                         \u001b[0mcomputable_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Graph disconnected: cannot obtain value for tensor Tensor(\"input_5:0\", shape=(?, ?), dtype=float32) at layer \"input_5\". The following previous layers were accessed without issue: []"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(16,))\n",
    "decoder_state_input_c = Input(shape=(16,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_embedding_x, initial_state=encoder_states)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, None, 8)      248         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) [(None, 16), (None,  1088        embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 16)           0           bidirectional_3[0][1]            \n",
      "                                                                 bidirectional_3[0][3]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 16)           0           bidirectional_3[0][2]            \n",
      "                                                                 bidirectional_3[0][4]            \n",
      "==================================================================================================\n",
      "Total params: 1,336\n",
      "Trainable params: 1,336\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, None, 8)      248         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 16)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 16)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   [(None, None, 16), ( 1600        embedding_6[0][0]                \n",
      "                                                                 input_9[0][0]                    \n",
      "                                                                 input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, None, 31)     527         lstm_6[2][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,375\n",
      "Trainable params: 2,375\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model.summary()\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_seq_to_num(sent):\n",
    "    output_sent = np.zeros((1,20),dtype=np.int32)\n",
    "    initial_list = []\n",
    "    sent = clean_text(sent)\n",
    "    for words in sent.split(' '):\n",
    "        if words in vocab_to_int_input:\n",
    "            initial_list.append(vocab_to_int_input[words])\n",
    "        else:\n",
    "            initial_list.append(vocab_to_int_input['<UNK>'])\n",
    "            \n",
    "    output_sent[0][0:len(initial_list)] = initial_list\n",
    "    output_sent[0] = output_sent[0][::-1]\n",
    "    return output_sent\n",
    "    \n",
    "#input_seq =  input_seq_to_num(\"hello how are you\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speak(input_sent):\n",
    "    states_value = encoder_model.predict(input_sent)\n",
    "    encoder_input_seq =  np.zeros((1,1), dtype=np.int32)\n",
    "    encoder_input_seq[0][0]  = 0\n",
    "    stop_condition = False\n",
    "    decoded_sent = []\n",
    "    max_decoder_seq_length = 10\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([encoder_input_seq] + states_value)\n",
    "\n",
    "        \n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        print sampled_token_index\n",
    "        sampled_char = sampled_token_index\n",
    "        decoded_sent.append(sampled_char)\n",
    "        if (sampled_char == 30 or len(decoded_sent) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "        encoder_input_seq =  np.zeros((1,1), dtype=np.int32)\n",
    "        encoder_input_seq[0][0]  = sampled_token_index\n",
    "        states_value = [h, c]\n",
    "    return \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "None\n",
      "[20 25 11  3 27]\n"
     ]
    }
   ],
   "source": [
    "print (speak(encoder_input_data[0]))\n",
    "print (encoder_input_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "30\n",
      "None\n",
      "11\n",
      "11\n",
      "30\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print (speak([19 ,25  ,4  ,4  ,3 ,29 ,22  ,4 ,30  ,0  ,0  ,0  ,0  ,0  ,0  ,0  ,0  ,0  ,0  ,0]))\n",
    "print (speak([26 ,30 ,19 ,28 ,14 ,18 ,26  ,0  ,0  ,0  ,0  ,0  ,0  ,0  ,0  ,0  ,0  ,0  ,0  ,0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
